# we can execute the code in a IPython Notebook
# when we start pyspark , the SparkContext automatically created
print sc

# number RDD
numbers = sc.parallelize([1,2,3,4,5,6])
print numbers

text = sc.textFile('./data/guttenberg.txt')
print text

print numbers.collect()
print text.take(2)
#------

# Calculate Logistics Regression

import random 
from math import exp

iterations = 100 # iterations for the gradient descent

def readPoint(line):
    parts = line.split(',')
    y = int(parts[0])
    x = [ float(parts[i]) for i in range(1, len(parts))]
    x.append(1.0) #calculate intercept
    return (x , y)

# we read the points
points = sc.textFile('./data/logistic.txt').map(readPoint).cache()
n = len(points.take(1)[0][0])
num_points = points.count()
print 'num features: {}, num points: {}'.format(n, num_points)

## 

learning_rate = 1.0 

#initialize the weights
w = [ random.random() * 2 -1 for i in xrange(n)]
print 'initial weights: {}'.format(w)

def compute_gradient(x, y, node_w):
    x_w = 0.0
    for i in xrange(len(x)):
        x_w += x[i] * node_w[i]
    if x_w > 10:
        k = 1.0 - y
    elif x_w < -10:
        k = -y
    else:
        k =  1.0 / (1.0 + exp(-x_w)) - y
    return [ x_i * k for x_i in x]
    
def add_gradient(g1, g2):
    for i in xrange(len(g1)):
        g1[i] += g2[i]
    return g1


for iteration in range(iterations):
        gradient = points.map(lambda (x, y): compute_gradient(x, y, w)).reduce(add_gradient)
        # update weights
        for i in xrange(n):
             w[i]  +=  - learning_rate / num_points * gradient[i]
             
print 'final weights: {}'.format(w)
